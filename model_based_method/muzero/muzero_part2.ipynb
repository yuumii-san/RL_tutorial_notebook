{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MuZero: Model-based RL (Part 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a series of notebooks to learn Muzero, which is a popular model-based reinforcement learning algorithm. \n",
    "\n",
    "In part 1, we learned the overview of Muzero as well as Monte Carlo Tree Search (MCTS) to collect training samples through self-play. In this notebook, we will learn the deep learning models used in MuZero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review of three models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we learned in part 1, Muzero uses three deep learning models to learn the dynamics of the environment as well as the optimal policy. They are:\n",
    "- Representation model: $s^0 = h_\\theta(o_t)$\n",
    "    - Input: raw state of the current root\n",
    "    - Output: latent state of the current root\n",
    "- Dynamics model: $r^k, s^k = g_\\theta(s^{k-1}, a^k)$\n",
    "    - Input: latent state, action to take\n",
    "    - Output: next latent state, expected immediate reward\n",
    "- Prediction model: $p^k, v^k = f_\\theta(s^k)$\n",
    "    - Input: latent state\n",
    "    - Output: policy at the input latent state, expected value at the input latent state\n",
    "where $t$ is the index for the past and current steps and $k$ is the index for the future steps.\n",
    "\n",
    "While dynamics model and prediction model used in original Muzero paper trained on multipel output values, we will devide these outputs into a different model to stablize the training process. More specifically, below code models each individual quantity using a separate network using five models.\n",
    "- Representation model:\n",
    "    - Input: raw state of the current root\n",
    "    - Output: latent state of the current root\n",
    "- Dynamic model:\n",
    "    - Input: latent state, action to take\n",
    "    - Output: next latent state\n",
    "- Reward model:\n",
    "    - Input: latent state, action to take\n",
    "    - Output: expected immediate reward\n",
    "- Value model:\n",
    "    - Input: latent state\n",
    "    - Output: expected value at the input latent state\n",
    "- Policy model:\n",
    "    - Input: latent state\n",
    "    - Output: policy at the input latent state\n",
    "\n",
    "\n",
    "The combination of the dynamics model and reward model behaves like the dynamics model of the original Muzero paper. The combination of the value model and policy model behaves like the prediction model of the original Muzero paper.\n",
    "\n",
    "Muzero learns all of these models at the same time. The loss function is defined as the sum of three errors:\n",
    "- Policy loss: the error between the actions predicted by the policy $p^k_t$ and by the search policy $\\pi_{t+k}$. \n",
    "- Value loss: the error between the value function $v^k_t$ and the value target, $z_{t+k}$\n",
    "- Reward loss: the error between the predicted immediate reward $r^k_t$ and the observed immediate reward $u_{t+k}$\n",
    "\n",
    "With the sum of three loss values, MuZero runs optimizer and gradient descent as we do for typical deep learning model training. \n",
    "Let's review each model one by one.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in part1, we assume using the CartPole-v0 environment in Gymnasium. The environment has two potential actions and each state is represented by a vector of four values (cart position, cart velocity, pole angle, and pole angular velocity). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_shape = 4\n",
    "action_size = 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representation network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first define a representation network. It receives a raw state of the current root node and returns its latent state. Thus, the input shape is the state shape. In the architecture used in the MuZero paper, the input will be transformed into the shape of the hidden neuron size. The outputs from hidden neurons are then transformed into the shape of embedding size to get the output latent state. The hidden neuron size and embedding size are the hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RepresentationNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_neurons, embedding_size):\n",
    "        super(RepresentationNetwork, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_neurons),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_neurons, embedding_size),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "    \n",
    "hidden_neurons = 48\n",
    "embedding_size = 4\n",
    "rep_net = RepresentationNetwork(input_size=state_shape, hidden_neurons=hidden_neurons, embedding_size=embedding_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamics network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dynamic network has a similar architecture to the representation network. But one difference is the input size. The dynamic function receives the latent state and action to take as input. In this tutorial, we use one-hot encoding to represent the action to take. For example, when the cart moves left, the action will be represented as [1,0]. On the other hand, when the cart moves right, the action will be represented as [0,1]. We combine these two-dimensional vectors with the embedded latent state. Thus, the input has the shape of embedding size + action size.\n",
    "The output is the next latent state reached by taking the input action at the input latent state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_neurons, embedding_size):\n",
    "        super(DynamicNetwork, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_neurons),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_neurons, embedding_size),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "    \n",
    "dyn_net = DynamicNetwork(input_size=embedding_size+action_size, hidden_neurons=hidden_neurons, embedding_size=embedding_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reward network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reward network receives the latent state and action to take as the input and returns the predicted immediate reward as the output. In a Cartpole environment, a reward of +1 is granted to the agent at each step while the pole is kept upright. Thus, the predicted immediate reward (output) is a scalar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewardNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_neurons):\n",
    "        super(RewardNetwork, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_neurons),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_neurons, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "    \n",
    "rew_net = RewardNetwork(input_size=embedding_size+action_size, hidden_neurons=hidden_neurons)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value network receives the latent state and returns the predicted expected value at the state. Instead of returning the output as a scalar value, MuZero uses an architecture to output multi-dimensional output and then applies an invertible transformation to get the predicted value (scalar). For more detail, please check \"Appendix F Network architecture\" of [the MuZero paper](https://arxiv.org/pdf/1911.08265#page=14.33) and \"Appendix A: Proposition A.2\" of [this paper](https://arxiv.org/pdf/1805.11593). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_neurons, value_support_size):\n",
    "        super(ValueNetwork, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_neurons),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_neurons, value_support_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "    \n",
    "def value_transform(self, value_support):\n",
    "    epsilon = 0.001\n",
    "    value = torch.nn.functional.softmax(value_support)\n",
    "    value = np.dot(value.detach().numpy(), range(len(value_support)))\n",
    "    value = np.sign(value) * (\n",
    "            ((np.sqrt(1 + 4 * epsilon\n",
    "                * (np.abs(value) + 1 + epsilon)) - 1) / (2 * epsilon)) ** 2 - 1\n",
    "    )\n",
    "    return value\n",
    "    \n",
    "max_value = 200\n",
    "value_support_size = math.ceil(math.sqrt(max_value)) + 1\n",
    "val_net = ValueNetwork(input_size=embedding_size, hidden_neurons=hidden_neurons, value_support_size=value_support_size)\n",
    "network_output = val_net(torch.Tensor([1,1,1,1])) # output from network (multi-dimensional)\n",
    "predicted_value = value_transform(output) # value after applying transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, the policy network receives the hidden state and returns the policy at the input state. This output value is not a probability. MuZero applies a softmax function to this output to get the probability of taking each action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_neurons, action_size):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_neurons),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_neurons, action_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "    \n",
    "pol_net = PolicyNetwork(input_size=embedding_size, hidden_neurons=hidden_neurons, action_size=action_size)\n",
    "policy_logits = val_net(torch.Tensor([1,1,1,1])) # output from network (multi-dimensional)\n",
    "softmax_policy = torch.nn.functional.softmax(torch.squeeze(policy_logits))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In part1, we skipped the detail of two functions, initial_inference and recurrent_inference, which were used to run Monte Carlo Tree Search (MCTS). Now, we are ready to cover them. We use initial_inference function to expand the current root node. What this function does is:\n",
    "- Use representation network to get the latent representation of the current root note\n",
    "- Use value network to get the expected value at the current latent state\n",
    "- Use policy network to get the policy at the current latent state\n",
    "\n",
    "In the below implementation, the InitialModel class integrates these three steps. Thus, in initial_inference function, we create the InitialModel object and use this to return the transformed scalar value, immediate reward (always set as 0 for the root state), policy before applying sigmoid transformation, and latent representation of the root state.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InitialModel(nn.Module):\n",
    "    def __init__(self, representation_network, value_network, policy_network):\n",
    "        super(InitialModel, self).__init__()\n",
    "        self.representation_network = representation_network\n",
    "        self.value_network = value_network\n",
    "        self.policy_network = policy_network\n",
    "\n",
    "    def forward(self, state):\n",
    "        hidden_representation = self.representation_network(state)\n",
    "        value = self.value_network(hidden_representation)\n",
    "        policy_logits = self.policy_network(hidden_representation)\n",
    "        return hidden_representation, value, policy_logits\n",
    "\n",
    "\n",
    "def initial_inference(state):\n",
    "    rep_net = RepresentationNetwork(input_size=state_shape, hidden_neurons=hidden_neurons, embedding_size=embedding_size)\n",
    "    val_net = ValueNetwork(input_size=embedding_size, hidden_neurons=hidden_neurons, value_support_size=value_support_size)\n",
    "    pol_net = PolicyNetwork(input_size=embedding_size, hidden_neurons=hidden_neurons, action_size=action_size)\n",
    "    \n",
    "    initial_model = InitialModel(rep_net, val_net, pol_net)\n",
    "    hidden_representation, value, policy_logits = initial_model(state)\n",
    "    return value_transform(value), 0, policy_logits, hidden_representation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurrent inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another function we used in MCTS is recurrent_inference function. This function is used to run the mental simulation during MCTS. What this function does is:\n",
    "- Use the dyamic network to get the next latent state when taking the input action at the input state\n",
    "- Use the reward network to get the immediate reward when taking the input action at the input state\n",
    "- Use the value network to get the expected value at the next latent state\n",
    "- Use the policy network to get the policy at the next latent state\n",
    "\n",
    "In the below implementation, the RecurrentModel class integrates these four steps. Thus, in recurrent_inference function, we create the RecurrentModel object and use this to return the transformed scalar value, immediate reward, policy before applying a sigmoid function, and latent representation of the next state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecurrentModel(nn.Module):\n",
    "    def __init__(self, dynamic_network, reward_network, value_network, policy_network):\n",
    "        super(RecurrentModel, self).__init__()\n",
    "        self.dynamic_network = dynamic_network\n",
    "        self.reward_network = reward_network\n",
    "        self.value_network = value_network\n",
    "        self.policy_network = policy_network\n",
    "\n",
    "    def forward(self, state_with_action):\n",
    "        hidden_representation = self.dynamic_network(state_with_action)\n",
    "        reward = self.reward_network(state_with_action)\n",
    "        value = self.value_network(hidden_representation)\n",
    "        policy_logits = self.policy_network(hidden_representation)\n",
    "        return hidden_representation, reward, value, policy_logits\n",
    "\n",
    "    \n",
    "def hidden_state_with_action(self, hidden_state, action):\n",
    "    \"\"\"\n",
    "    Merge hidden state and one hot encoded action\n",
    "    \"\"\"\n",
    "    hidden_state_with_action = torch.concat(\n",
    "        (hidden_state, torch.tensor(self._action_to_one_hot(action, self.action_size))[0]), axis=0)\n",
    "    return hidden_state_with_action\n",
    "\n",
    "def recurrent_inference(hidden_state, action):\n",
    "    dyn_net = DynamicNetwork(input_size=embedding_size+action_size, hidden_neurons=hidden_neurons, embedding_size=embedding_size)\n",
    "    rew_net = RewardNetwork(input_size=embedding_size+action_size, hidden_neurons=hidden_neurons)\n",
    "    val_net = ValueNetwork(input_size=embedding_size, hidden_neurons=hidden_neurons, value_support_size=value_support_size)\n",
    "    pol_net = PolicyNetwork(input_size=embedding_size, hidden_neurons=hidden_neurons, action_size=action_size)\n",
    "    \n",
    "    state_with_action = hidden_state_with_action(hidden_state, action)\n",
    "    recurrent_model = RecurrentModel(dyn_net, rew_net, val_net, pol_net)\n",
    "    hidden_representation, reward, value, policy_logits = recurrent_model(state_with_action)\n",
    "    return value_transform(value), reward, policy_logits, hidden_representation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we reviewed the deep neural networks used in MuZero. These networks are trained using the data collected with MCTC, which is the process we learned in part1. In the next notebook, we combine part1 and part2, and then add a few more elements to complete MuZero framework.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DRLhw2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
