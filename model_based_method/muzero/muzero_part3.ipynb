{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MuZero: Model-based RL (Part 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In part 1, we learned Monte Carlo Tree Search to collect training data. In part 2, we learned the deep learning models used in MuZero. In this notebook, we will integrate these two main components and other components to run the entire Muzero algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import collections\n",
    "from collections import deque\n",
    "import gymnasium as gym\n",
    "import itertools\n",
    "import random\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we define an environment which MuZero interacts with. As in part1 and part2, we use the CartPole environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment MuZero is interacting with.\n",
    "env = gym.make('CartPole-v0')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define several classes to store key information in MuZero. The first class we define is a Node class to store the information during MCTS self-play. This is the same code as the one we learned in part1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node(object):\n",
    "    \n",
    "    def __init__(self, prior):\n",
    "        \"\"\"\n",
    "        Node in MCTS\n",
    "        prior: The prior policy on the node, computed from policy network\n",
    "        \"\"\"\n",
    "        self.visit_count = 0\n",
    "        self.prior = prior\n",
    "        self.value_sum = 0\n",
    "        self.children = {}\n",
    "        self.hidden_representation = None\n",
    "        self.reward = 0\n",
    "        self.expanded = False\n",
    "\n",
    "    def value(self):\n",
    "        \"\"\"\n",
    "        Compute expected value of a node\n",
    "        \"\"\"\n",
    "        if self.visit_count == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return self.value_sum / self.visit_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Game class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define a game class that stores a single episode of interaction with the environment. Using store_search_statistics function, MuZero stores the experience in this Game class object. MuZero stores this class's information in a replay buffer.\n",
    "\n",
    "For the model training, make_target function is used to create the target data including target value, reward, and policy at each state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Game:\n",
    "    \"\"\"\n",
    "    A single episode of interaction with the environment.\n",
    "    \"\"\"\n",
    "    def __init__(self, action_space_size, discount, curr_state):\n",
    "\n",
    "        self.action_space_size = action_space_size\n",
    "        self.curr_state = curr_state\n",
    "        self.done = False\n",
    "        self.discount = discount\n",
    "        self.priorities = None\n",
    "\n",
    "        self.state_history = [self.curr_state]\n",
    "        self.action_history = []\n",
    "        self.reward_history = []\n",
    "\n",
    "        self.root_values = []\n",
    "        self.child_visits = []\n",
    "\n",
    "    def store_search_statistics(self, root):\n",
    "        \"\"\"\n",
    "        Stores the search statistics for the current root node\n",
    "        \n",
    "        root: Node object including the infomration of the current root node\n",
    "        \"\"\"\n",
    "        # Stores the normalized root node child visits (i.e. policy target)\n",
    "        sum_visits = sum(child.visit_count for child in root.children.values())\n",
    "        self.child_visits.append(np.array([\n",
    "            root.children[a].visit_count\n",
    "            / sum_visits if a in root.children else 0\n",
    "            for a in range(self.action_space_size)\n",
    "        ]))\n",
    "        \n",
    "        # Stores the root node value, computed from the MCTS (i.e. vlaue target)\n",
    "        self.root_values.append(root.value())\n",
    "\n",
    "    def take_action(self, action, env):\n",
    "        \"\"\"\n",
    "        Take an action and store the action, reward, and new state into history\n",
    "        \"\"\"\n",
    "        observation, reward, terminated, truncated, _ = env.step(action)\n",
    "        self.curr_state = observation\n",
    "        self.action_history.append(action)\n",
    "        self.reward_history.append(reward)\n",
    "        self.done = terminated | truncated\n",
    "        if not self.done:\n",
    "            self.state_history.append(self.curr_state)\n",
    "\n",
    "    def make_target(self, state_index, num_unroll_steps, td_steps):\n",
    "        \"\"\"\n",
    "        Makes the target data for training\n",
    "\n",
    "        state_index: the start state\n",
    "        num_unroll_steps: how many times to unroll from the current state\n",
    "                          each unroll forms a new target\n",
    "        td_steps: the number of td steps used in bootstrapping the value function\n",
    "        \"\"\"\n",
    "        targets = [] # target = (value, reward, policy)\n",
    "        actions = []\n",
    "\n",
    "        for current_index in range(state_index, state_index + num_unroll_steps + 1):\n",
    "            bootstrap_index = current_index + td_steps\n",
    "\n",
    "            # target value of the current node is the sum of 1) discounted rewards up to bootstrap index + 2) discounted value at bootstrap index            \n",
    "            \n",
    "            # compute 2)\n",
    "            # assign value=0 if bootstrap_index is after the end of episode\n",
    "            # otherwise, assign discounted value at bootstrap_index state\n",
    "            if bootstrap_index < len(self.root_values):\n",
    "                value = self.root_values[bootstrap_index][0] * (self.discount**td_steps)\n",
    "            else:\n",
    "                value = 0\n",
    "            \n",
    "            # compute 1)  \n",
    "            # add discounted reward values earned between current_index and bootstrap_index\n",
    "            for i, reward in enumerate(self.reward_history[current_index:bootstrap_index]):\n",
    "                value += reward * (self.discount**i)\n",
    "\n",
    "            # if current_index is after the end of episode, assign 0 as last_reward\n",
    "            # otherwise, assign the reward from last step as last_reward, which will be used as reward target\n",
    "            if current_index > 0 and current_index <= len(self.reward_history):\n",
    "                last_reward = self.reward_history[current_index-1]\n",
    "            else:\n",
    "                last_reward = 0\n",
    "                \n",
    "            if current_index < len(self.root_values): # current_index is within the episode, \n",
    "                targets.append((value, last_reward,\n",
    "                                self.child_visits[current_index]))\n",
    "                actions.append(self.action_history[current_index])\n",
    "            else: # current_index is after the end of episode\n",
    "                # State which pasts the end of the game are treated as an absorbing state.\n",
    "                num_actions = self.action_space_size\n",
    "                targets.append(\n",
    "                    (0, last_reward, np.array([1.0 / num_actions for _ in range(num_actions)]))) # assign value 0 and uniform policy\n",
    "                actions.append(np.random.choice(num_actions)) # assign a random action\n",
    "        return targets, actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replay Buffer class "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define a replay buffer class. MuZero stores game episodes in the  replay buffer. When training a model, MuZero samples the stored episodes from this buffer using sample_batch function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "    \"\"\"\n",
    "    Store training data acquired through self-play\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        self.buffer_size = config['buffer_size']\n",
    "        self.batch_size = config['batch_size']\n",
    "        self.buffer = deque(maxlen=self.buffer_size) # deque: list-like container with fast appends and pops on either end\n",
    "        self.td_steps = config['td_steps']\n",
    "        self.unroll_steps = config['num_unroll_steps']\n",
    "\n",
    "    def save_game(self, game):\n",
    "        \"\"\"\n",
    "        Save a game into replay buffer.\n",
    "        Max number of games saved in the buffer is defined as self.buffer_size\n",
    "        \"\"\"\n",
    "        if len(self.buffer) > self.buffer_size:\n",
    "            self.buffer.pop(0)\n",
    "        self.buffer.append(game)\n",
    "\n",
    "    def sample_batch(self):\n",
    "        \"\"\"\n",
    "        Sample batch_size games, along with an associated start position in each game\n",
    "        Make the targets for the batch to be used in training\n",
    "        \"\"\"\n",
    "        games = [self.sample_game() for _ in range(self.batch_size)] # randomly sample batch_size games\n",
    "        game_pos = [self.sample_position(g) for g in games] # randomly sample position from the game\n",
    "        batch = []\n",
    "        for (g, i) in zip(games, game_pos):\n",
    "            # create training targets (output) and actions (input)\n",
    "            targets, actions = g.make_target(\n",
    "                i, self.unroll_steps, self.td_steps) # each target = (value, reward, policy)\n",
    "            batch.append(\n",
    "                (g.state_history[i], actions, targets))\n",
    "        state_batch, actions_batch, targets_batch = zip(*batch) # unpack batch\n",
    "        actions_batch = list(zip(*actions_batch)) # unpack action\n",
    "        targets_init_batch, *targets_recurrent_batch = zip(*targets_batch) # unpack targets_batch, targets_init_batch: initial target, targets_recurrent_batch: subsequent targets\n",
    "        # * operator is used for extended unpacking, meaning that any additional targets beyond the initial one are packed into targets_recurrent_batch.\n",
    "        batch = (state_batch, targets_init_batch, targets_recurrent_batch,\n",
    "                 actions_batch)\n",
    "\n",
    "        return batch\n",
    "\n",
    "    def sample_game(self):\n",
    "        \"\"\"\n",
    "        Ramdonly sample a game from buffer\n",
    "        \"\"\"\n",
    "        game = np.random.choice(self.buffer)\n",
    "        return game\n",
    "\n",
    "    def sample_position(self, game):\n",
    "        \"\"\"\n",
    "        Randomply sample position from a game to start unrolling\n",
    "        \"\"\"\n",
    "        sampled_index = np.random.randint(\n",
    "            len(game.reward_history)-self.unroll_steps) # limit the sample in the space where we can unroll # of unroll_steps\n",
    "        return sampled_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Networks class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Networks class is used to instantiate five deep learning models we covered in part2. We will use the same model architecture as part 2. Networks class includes various helper functions such as initial_inference, recurrent_inference, and _value_transform to run MCTS and train networks.\n",
    "\n",
    "While _value_transform function transforms a multi-dimensional output from the value network into a scalar predicted value, _scalar_to_support function performs the inverse transformation - it transforms a scalar target value into a multi-dimensional value to train the value network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RepresentationNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Input: raw state of the current root\n",
    "    Output: latent state of the current root\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_neurons, embedding_size):\n",
    "        super(RepresentationNetwork, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_neurons),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_neurons, embedding_size),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "    \n",
    "    \n",
    "class ValueNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Input: latent state\n",
    "    Output: expected value at the input latent state\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_neurons, value_support_size):\n",
    "        super(ValueNetwork, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_neurons),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_neurons, value_support_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "    \n",
    "    \n",
    "class PolicyNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Input: latent state\n",
    "    Output: policy at the input latent state\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_neurons, action_size):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_neurons),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_neurons, action_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "    \n",
    "    \n",
    "class DynamicNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Input: latent state & action to take\n",
    "    Output: next latent state\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_neurons, embedding_size):\n",
    "        super(DynamicNetwork, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_neurons),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_neurons, embedding_size),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "    \n",
    "    \n",
    "class RewardNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Input: latent state & action to take\n",
    "    Output: expected immediate reward\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_neurons):\n",
    "        super(RewardNetwork, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_neurons),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_neurons, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "    \n",
    "    \n",
    "class InitialModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Combine Representation, Value, and Policy networks\n",
    "    \"\"\"\n",
    "    def __init__(self, representation_network, value_network, policy_network):\n",
    "        super(InitialModel, self).__init__()\n",
    "        self.representation_network = representation_network\n",
    "        self.value_network = value_network\n",
    "        self.policy_network = policy_network\n",
    "\n",
    "    def forward(self, state):\n",
    "        hidden_representation = self.representation_network(state)\n",
    "        value = self.value_network(hidden_representation)\n",
    "        policy_logits = self.policy_network(hidden_representation)\n",
    "        return hidden_representation, value, policy_logits\n",
    "\n",
    "\n",
    "class RecurrentModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Combine Dynamic, Reward, Value, and Policy network\n",
    "    \"\"\"\n",
    "    def __init__(self, dynamic_network, reward_network, value_network, policy_network):\n",
    "        super(RecurrentModel, self).__init__()\n",
    "        self.dynamic_network = dynamic_network\n",
    "        self.reward_network = reward_network\n",
    "        self.value_network = value_network\n",
    "        self.policy_network = policy_network\n",
    "\n",
    "    def forward(self, state_with_action):\n",
    "        hidden_representation = self.dynamic_network(state_with_action)\n",
    "        reward = self.reward_network(state_with_action)\n",
    "        value = self.value_network(hidden_representation)\n",
    "        policy_logits = self.policy_network(hidden_representation)\n",
    "        return hidden_representation, reward, value, policy_logits\n",
    "    \n",
    "    \n",
    "class Networks(nn.Module):\n",
    "    \"\"\"\n",
    "    Create both InitialModel and RecurrentModel class objects \n",
    "    and helper functions to run MCTS and train models\n",
    "    \"\"\"\n",
    "    def __init__(self, representation_network, value_network, policy_network, dynamic_network, reward_network, max_value):\n",
    "        super().__init__()\n",
    "        self.train_steps = 0\n",
    "        self.action_size = 2\n",
    "        self.representation_network = representation_network\n",
    "        self.value_network = value_network\n",
    "        self.policy_network = policy_network\n",
    "        self.dynamic_network = dynamic_network\n",
    "        self.reward_network = reward_network\n",
    "        self.initial_model = InitialModel(self.representation_network, self.value_network, self.policy_network)\n",
    "        self.recurrent_model = RecurrentModel(self.dynamic_network, self.reward_network, self.value_network,\n",
    "                                              self.policy_network)\n",
    "        self.value_support_size = math.ceil(math.sqrt(max_value)) + 1\n",
    "\n",
    "    def initial_inference(self, state):\n",
    "        hidden_representation, value, policy_logits = self.initial_model(state)\n",
    "        assert isinstance(self._value_transform(value), float)\n",
    "        return self._value_transform(value), 0, policy_logits, hidden_representation\n",
    "\n",
    "    def recurrent_inference(self, hidden_state, action):\n",
    "        hidden_state_with_action = self._hidden_state_with_action(hidden_state, action)\n",
    "        hidden_representation, reward, value, policy_logits = self.recurrent_model(hidden_state_with_action)\n",
    "        return self._value_transform(value), self._reward_transform(reward), policy_logits, hidden_representation\n",
    "\n",
    "    def _value_transform(self, value_support):\n",
    "        \"\"\"\n",
    "        Apply invertable transformation to get a numpy scalar value\n",
    "        \"\"\"\n",
    "        epsilon = 0.001\n",
    "        value = torch.nn.functional.softmax(value_support)\n",
    "        value = np.dot(value.detach().numpy(), range(self.value_support_size))\n",
    "        value = np.sign(value) * (\n",
    "                ((np.sqrt(1 + 4 * epsilon\n",
    "                 * (np.abs(value) + 1 + epsilon)) - 1) / (2 * epsilon)) ** 2 - 1\n",
    "        )\n",
    "        return value\n",
    "\n",
    "    def _reward_transform(self, reward):\n",
    "        \"\"\"\n",
    "        Transform reward into a numpy scalar value\n",
    "        \"\"\"\n",
    "        return reward.detach().cpu().numpy()  # Assuming reward is a PyTorch tensor\n",
    "\n",
    "    def _hidden_state_with_action(self, hidden_state, action):\n",
    "        \"\"\"\n",
    "        Merge hidden state and one hot encoded action\n",
    "        \"\"\"\n",
    "        hidden_state_with_action = torch.concat(\n",
    "            (hidden_state, torch.tensor(self._action_to_one_hot(action, self.action_size))[0]), axis=0)\n",
    "        return hidden_state_with_action\n",
    "    \n",
    "    def _action_to_one_hot(self, action, action_space_size):\n",
    "        \"\"\"\n",
    "        Compute one hot of action to be combined with state representation\n",
    "        \"\"\"\n",
    "        return np.array([1 if i == action else 0 for i in range(action_space_size)]).reshape(1, -1)\n",
    "    \n",
    "    def _scalar_to_support(self, target_value):\n",
    "        \"\"\"\n",
    "        Transform value into a multi-dimensional target value to train a network\n",
    "        \"\"\"\n",
    "        batch = target_value.size(0)\n",
    "        targets = torch.zeros((batch, self.value_support_size))\n",
    "        target_value = torch.sign(target_value) * \\\n",
    "            (torch.sqrt(torch.abs(target_value) + 1)\n",
    "            - 1 + 0.001 * target_value)\n",
    "        target_value = torch.clamp(target_value, 0, self.value_support_size)\n",
    "        floor = torch.floor(target_value)\n",
    "        rest = target_value - floor\n",
    "        targets[torch.arange(batch, dtype=torch.long), floor.long()] = 1 - rest\n",
    "        indexes = floor.long() + 1\n",
    "        mask = indexes < self.value_support_size\n",
    "        batch_mask = torch.arange(batch)[mask]\n",
    "        rest_mask = rest[mask]\n",
    "        index_mask = indexes[mask]\n",
    "        targets[batch_mask, index_mask] = rest_mask\n",
    "        return targets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code defines several functions to train the deep learning models.\n",
    "\n",
    "train_network function trains the networks by using the batch data sampled from the replay buffer. It calls update_weights function, which performs all steps of network training. To maintain a roughly similar magnitude of the gradient across a different number of unroll steps during MCTS, MuZero scales the gradient with scale_gradient function. For more details on gradient scaling, please refer to [Appendinx G of MuZero paper](https://arxiv.org/pdf/1911.08265#page=14.33). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_gradient(tensor, scale):\n",
    "    \"\"\"\n",
    "    Function to scale gradient as described in MuZero Appendix\n",
    "    \"\"\"\n",
    "    return tensor * scale + tensor.detach() * (1. - scale)\n",
    "\n",
    "\n",
    "def train_network(config, network, replay_buffer, optimizer, train_log):\n",
    "    \"\"\"\n",
    "    Train Networks\n",
    "    \"\"\"\n",
    "    for _ in range(config['train_per_epoch']):\n",
    "        batch = replay_buffer.sample_batch()\n",
    "        update_weights(config, network, optimizer, batch, train_log)\n",
    "\n",
    "\n",
    "def update_weights(config, network, optimizer, batch, train_log):\n",
    "    \"\"\"\n",
    "    Train networks by sampling games from repay buffer\n",
    "    config: dictionary specifying parameter configurations\n",
    "    network: network class to train\n",
    "    optimizer: optimizer used to update the network_model weights\n",
    "    batch: batch of experience\n",
    "    train_log: class to store the train results\n",
    "    \"\"\"\n",
    "    # for every game in sample batch, unroll and update network_model weights\n",
    "    def loss():\n",
    "        mse = torch.nn.MSELoss()\n",
    "\n",
    "        loss = 0\n",
    "        total_value_loss = 0\n",
    "        total_reward_loss = 0\n",
    "        total_policy_loss = 0\n",
    "        (state_batch, targets_init_batch, targets_recurrent_batch,\n",
    "         actions_batch) = batch\n",
    "\n",
    "        state_batch = torch.tensor(state_batch)\n",
    "\n",
    "        # get prediction from initial model (i.e. combination of dynamic, value, and policy networks)\n",
    "        hidden_representation, initial_values, policy_logits = network.initial_model(state_batch)\n",
    "\n",
    "        # create a value and policy target from batch data\n",
    "        target_value_batch, _, target_policy_batch = zip(*targets_init_batch) # (value, reward, policy)\n",
    "        target_value_batch = torch.tensor(target_value_batch).float()\n",
    "        target_value_batch = network._scalar_to_support(target_value_batch) # transform into a multi-dimensional target\n",
    "\n",
    "        # compute the error for the initial inference\n",
    "        # reward error is always 0 for initial inference\n",
    "        value_loss = F.cross_entropy(initial_values, target_value_batch)\n",
    "        policy_loss = F.cross_entropy(policy_logits, torch.tensor(target_policy_batch))\n",
    "        loss = 0.25 * value_loss + policy_loss\n",
    "\n",
    "        total_value_loss = 0.25 * value_loss.item()\n",
    "        total_policy_loss = policy_loss.item()\n",
    "\n",
    "        # unroll batch with recurrent inference and accumulate loss\n",
    "        for actions_batch, targets_batch in zip(actions_batch, targets_recurrent_batch):\n",
    "            target_value_batch, target_reward_batch, target_policy_batch = zip(*targets_batch)\n",
    "\n",
    "            # get prediction from recurrent_model (i.e. dynamic, reward, value, and policy networks)\n",
    "            actions_batch_onehot = F.one_hot(torch.tensor(actions_batch), num_classes=network.action_size).float()\n",
    "            state_with_action = torch.cat((hidden_representation, actions_batch_onehot), dim=1)\n",
    "            hidden_representation, rewards, values, policy_logits = network.recurrent_model(state_with_action)\n",
    "\n",
    "            # create a value, policy, and reward target from batch data\n",
    "            target_value_batch = torch.tensor(target_value_batch).float()\n",
    "            target_value_batch = network._scalar_to_support(target_value_batch)\n",
    "            target_policy_batch = torch.tensor(target_policy_batch).float()\n",
    "            target_reward_batch = torch.tensor(target_reward_batch).float()\n",
    "\n",
    "            # compute the loss for recurrent_inference \n",
    "            value_loss = F.cross_entropy(values, target_value_batch)\n",
    "            policy_loss = F.cross_entropy(policy_logits, target_policy_batch)\n",
    "            reward_loss = mse(rewards, target_reward_batch)\n",
    "\n",
    "            # accumulate loss\n",
    "            loss_step = (0.25 * value_loss + reward_loss + policy_loss)\n",
    "            total_value_loss += 0.25 * value_loss.item()\n",
    "            total_policy_loss += policy_loss.item()\n",
    "            total_reward_loss += reward_loss.item()\n",
    "                        \n",
    "            # gradient scaling\n",
    "            gradient_loss_step = scale_gradient(loss_step,(1/config['num_unroll_steps']))\n",
    "            loss += gradient_loss_step\n",
    "            scale = 0.5\n",
    "            hidden_representation = hidden_representation / scale\n",
    "            \n",
    "        # store loss result for plotting\n",
    "        train_log.total_losses.append(loss.item())\n",
    "        train_log.value_losses.append(total_value_loss)\n",
    "        train_log.policy_losses.append(total_policy_loss)\n",
    "        train_log.reward_losses.append(total_reward_loss)\n",
    "        return loss\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss = loss()\n",
    "    loss.backward() # Compute gradients of loss with respect to parameters\n",
    "    optimizer.step() # Update parameters based on gradients\n",
    "    network.train_steps += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MCTS class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run MCTS, we summarize functions we learned in part1 such as run_mcts into MCTS class. This class contains various helpers to run self-play."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCTS():\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        \n",
    "    def run_mcts(self, config, root, network, min_max_stats):\n",
    "        \"\"\"\n",
    "        Run the main loop of MCTS for config['num_simulations'] simulations\n",
    "\n",
    "        root: the root node\n",
    "        network: the network\n",
    "        min_max_stats: the min max stats object\n",
    "        \"\"\"\n",
    "        for i in range(config['num_simulations']):\n",
    "            history = []\n",
    "            node = root\n",
    "            search_path = [node]\n",
    "\n",
    "            # expand node until reaching the leaf node\n",
    "            while node.expanded:\n",
    "                action, node = self.select_child(config, node, min_max_stats)\n",
    "                history.append(action)\n",
    "                search_path.append(node)\n",
    "            parent = search_path[-2]\n",
    "            action = history[-1]\n",
    "            \n",
    "            # expand the leaf node\n",
    "            value = self.expand_node(node, list(\n",
    "                range(config['action_space_size'])), network, parent.hidden_representation, action)\n",
    "            \n",
    "            # perform backpropagation\n",
    "            self.backpropagate(search_path, value,\n",
    "                        config['discount'], min_max_stats)\n",
    "\n",
    "\n",
    "    def select_action(self, config, node, test=False):\n",
    "        \"\"\"\n",
    "        Select an action to take\n",
    "        train mode: action selection is performed stochastically (softmax)\n",
    "        test mode: action selection is performed deterministically (argmax)\n",
    "        \"\"\"\n",
    "        visit_counts = [\n",
    "            (child.visit_count, action) for action, child in node.children.items()\n",
    "        ]\n",
    "        if not test:\n",
    "            t = config['visit_softmax_temperature_fn']\n",
    "            action = self.softmax_sample(visit_counts, t)\n",
    "        else:\n",
    "            action = self.softmax_sample(visit_counts, 0)\n",
    "        return action\n",
    "\n",
    "\n",
    "    def select_child(self, config, node, min_max_stats):\n",
    "        \"\"\"\n",
    "        Select a child at an already expanded node\n",
    "        Selection is based on the UCB score\n",
    "        \"\"\"\n",
    "        best_action, best_child = None, None\n",
    "        ucb_compare = -np.inf\n",
    "        for action,child in node.children.items():\n",
    "            ucb = self.compute_ucb_score(config, node, child, min_max_stats)\n",
    "            if ucb > ucb_compare:\n",
    "                ucb_compare = ucb\n",
    "                best_action = action # action, int\n",
    "                best_child = child # node object\n",
    "        return best_action, best_child\n",
    "\n",
    "\n",
    "    def compute_ucb_score(self, config, parent, child, min_max_stats):\n",
    "        \"\"\"\n",
    "        Compute UCB Score of a child given the parent statistics\n",
    "        Appendix B of MuZero paper\n",
    "        \"\"\"\n",
    "        pb_c = np.log((parent.visit_count + config['pb_c_base'] + 1)\n",
    "                    / config['pb_c_base']) + config['pb_c_init']\n",
    "        pb_c *= np.sqrt(parent.visit_count) / (child.visit_count + 1)\n",
    "\n",
    "        prior_score = pb_c*child.prior.detach().numpy()\n",
    "        if child.visit_count > 0:\n",
    "            value_score = min_max_stats.normalize(\n",
    "                child.reward + config['discount']*child.value())\n",
    "        else:\n",
    "            value_score = 0\n",
    "        return prior_score + value_score\n",
    "\n",
    "\n",
    "    def expand_root(self, node, actions, network, current_state):\n",
    "        \"\"\"\n",
    "        Expand the root node given the current state\n",
    "        \"\"\"\n",
    "        # obtain the latent state, policy, and value of the root node \n",
    "        # by using a InitialModel\n",
    "        observation = torch.tensor(current_state)\n",
    "        transformed_value, reward, policy_logits, hidden_representation = network.initial_inference(observation)\n",
    "        node.hidden_representation = hidden_representation\n",
    "        node.reward = reward # always 0 for initial inference\n",
    "\n",
    "        # extract softmax policy and set node.policy\n",
    "        softmax_policy = torch.nn.functional.softmax(torch.squeeze(policy_logits))\n",
    "        node.policy = softmax_policy\n",
    "\n",
    "        # instantiate node's children with prior values, obtained from the predicted policy\n",
    "        for action, prob in zip(actions, softmax_policy):\n",
    "            child = Node(prob)\n",
    "            node.children[action] = child\n",
    "\n",
    "        # set node as expanded\n",
    "        node.expanded = True\n",
    "        \n",
    "        return transformed_value\n",
    "\n",
    "\n",
    "    def expand_node(self, node, actions, network, parent_state, parent_action):\n",
    "        \"\"\"\n",
    "        Expand a leaf node given the parent state and action\n",
    "        \"\"\"\n",
    "        # run recurrent inference at the leaf node\n",
    "        transformed_value, reward, policy_logits, hidden_representation = network.recurrent_inference(parent_state, parent_action)\n",
    "        node.hidden_representation = hidden_representation\n",
    "        node.reward = reward\n",
    "\n",
    "        # compute softmax policy and store it to node.policy\n",
    "        softmax_policy = torch.nn.functional.softmax(torch.squeeze(policy_logits))\n",
    "        node.policy = softmax_policy\n",
    "\n",
    "        # instantiate node's children with prior values, obtained from the predicted softmax policy\n",
    "        for action, prob in zip(actions,softmax_policy):\n",
    "            child = Node(prob)\n",
    "            node.children[action] = child\n",
    "\n",
    "        # set node as expanded\n",
    "        node.expanded = True\n",
    "        \n",
    "        return transformed_value\n",
    "\n",
    "\n",
    "    def add_exploration_noise(self, config, node):\n",
    "        \"\"\"\n",
    "        Add exploration noise by adding dirichlet noise to the prior over children\n",
    "        This is governed by root_dirichlet_alpha and root_exploration_fraction\n",
    "        \"\"\"\n",
    "        actions = list(node.children.keys())\n",
    "        noise = np.random.dirichlet([config['root_dirichlet_alpha']]*len(actions))\n",
    "        frac = config['root_exploration_fraction']\n",
    "        for a, n in zip(actions, noise):\n",
    "            node.children[a].prior = node.children[a].prior * (1-frac) + n*frac\n",
    "\n",
    "\n",
    "    def backpropagate(self, path, value, discount, min_max_stats):\n",
    "        \"\"\"\n",
    "        Update a discounted total value and total visit count\n",
    "        \"\"\"\n",
    "        for node in reversed(path):\n",
    "            node.visit_count += 1\n",
    "            node.value_sum += value \n",
    "            min_max_stats.update(node.value())\n",
    "            value = node.reward + discount * value\n",
    "\n",
    "\n",
    "    def softmax_sample(self, visit_counts, temperature):\n",
    "        \"\"\"\n",
    "        Sample an action\n",
    "        \"\"\"\n",
    "        counts_arr = np.array([c[0] for c in visit_counts])\n",
    "        if temperature == 0: # argmax\n",
    "            action_idx = np.argmax(counts_arr)\n",
    "        else: # softmax\n",
    "            numerator = np.power(counts_arr,1/temperature)\n",
    "            denominator = np.sum(numerator)\n",
    "            dist = numerator / denominator\n",
    "            action_idx = np.random.choice(np.arange(len(counts_arr)),p=dist)\n",
    "\n",
    "        return action_idx\n",
    "    \n",
    "    \n",
    "class MinMaxStats(object):\n",
    "    \"\"\"\n",
    "    Store the min-max values of the environment to normalize the values\n",
    "    Max value will be 1 and min value will be 0\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, minimum, maximum):\n",
    "        self.maximum = maximum\n",
    "        self.minimum = minimum\n",
    "\n",
    "    def update(self, value: float):\n",
    "        self.maximum = max(self.maximum, value)\n",
    "        self.minimum = min(self.minimum, value)\n",
    "\n",
    "    def normalize(self, value: float) -> float:\n",
    "        if self.maximum > self.minimum:\n",
    "            # We normalize only when we have set the maximum and minimum values.\n",
    "            return (value - self.minimum) / (self.maximum - self.minimum)\n",
    "        return value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper classes to store and plot train/test performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define two optional classes to store training loss and test reward at each epoch for plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainLogs(object):\n",
    "    def __init__(self):\n",
    "        self.value_losses = []\n",
    "        self.reward_losses = []\n",
    "        self.policy_losses = []\n",
    "        self.total_losses = []\n",
    "\n",
    "    def plot_total_loss(self):\n",
    "        x = np.arange(len(self.total_losses))\n",
    "        plt.figure()\n",
    "        plt.plot(x, self.total_losses, label=\"Train Loss\", color='k')\n",
    "        plt.xlabel(\"Train Steps\", fontsize=15)\n",
    "        plt.ylabel(\"Loss\", fontsize=15)\n",
    "        plt.show()\n",
    "        # plt.savefig('./RL/ModelBasedML/figure/total_loss.png')\n",
    "\n",
    "    def plot_individual_losses(self):\n",
    "        x = np.arange(len(self.total_losses))\n",
    "        plt.figure()\n",
    "        plt.plot(x, self.value_losses, label=\"Value Loss\", color='r')\n",
    "        plt.plot(x, self.policy_losses, label=\"Policy Loss\", color='b')\n",
    "        plt.plot(x, self.reward_losses, label=\"Reward Loss\", color='g')\n",
    "        plt.xlabel(\"Train Steps\", fontsize=15)\n",
    "        plt.ylabel(\"Losses\", fontsize=15)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        # plt.savefig('./RL/ModelBasedML/figure/individual_loss.png')\n",
    "\n",
    "\n",
    "class TestLogs(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.test_log = []\n",
    "\n",
    "    def add_reward(self, reward):\n",
    "        self.test_log.append(reward)\n",
    "        \n",
    "    def plot_rewards(self):\n",
    "        x = np.arange(len(self.test_log))\n",
    "        plt.figure()\n",
    "        plt.plot(x, self.test_log, label=\"Test Reward\", color='orange')\n",
    "        plt.xlabel(\"Test Episodes\", fontsize=15)\n",
    "        plt.ylabel(\"Reward\", fontsize=15)\n",
    "        plt.show()\n",
    "        # plt.savefig('./RL/ModelBasedML/figure/test_reward.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's define the main function of MuZero (self_play function) by integrating everything we defined so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_play(env, config, replay_buffer, network):\n",
    "    # create objects to store data for plotting\n",
    "    test_log = TestLogs()\n",
    "    train_log = TrainLogs()\n",
    "    \n",
    "    # create optimizer for training\n",
    "    optimizer = torch.optim.Adam(network.parameters(), lr=config['lr_init'])\n",
    "    \n",
    "    # self-play and network training iterations\n",
    "    for i in range(config['num_epochs']):  # Number of Steps of train/play alternations\n",
    "        print(f\"===Epoch Number {i}===\")\n",
    "        score = play_games(\n",
    "            config, replay_buffer, network, env)\n",
    "        print(\"Average traininig score:\", score)\n",
    "        train_network(config, network, replay_buffer, optimizer, train_log)\n",
    "        print(\"Average test score:\", test(config, network, env, test_log))\n",
    "\n",
    "    # plot\n",
    "    train_log.plot_individual_losses()\n",
    "    train_log.plot_total_loss()\n",
    "    test_log.plot_rewards()\n",
    "\n",
    "\n",
    "def play_games(config, replay_buffer, network, env):\n",
    "    \"\"\"\n",
    "    Play multiple games and store them in the replay buffer\n",
    "    \"\"\"\n",
    "    returns = 0\n",
    "\n",
    "    for _ in range(config['games_per_epoch']):\n",
    "        game = play_game(config, network, env)\n",
    "        replay_buffer.save_game(game)\n",
    "        returns += sum(game.reward_history)\n",
    "\n",
    "    return returns / config['games_per_epoch']\n",
    "\n",
    "\n",
    "def play_game(config, network, env):\n",
    "    \"\"\"\n",
    "    Plays one game\n",
    "    \"\"\"\n",
    "    # Initialize environment\n",
    "    start_state, _ = env.reset()\n",
    "    \n",
    "    game = Game(config['action_space_size'], config['discount'], start_state)        \n",
    "    mcts = MCTS(config)\n",
    "    \n",
    "    # Play a game using MCTS until game will be done or max_moves will be reached\n",
    "    while not game.done and len(game.action_history) < config['max_moves']:\n",
    "        root = Node(0)\n",
    "        \n",
    "        # Create MinMaxStats Object to normalize values\n",
    "        min_max_stats = MinMaxStats(config['min_value'], config['max_value'])\n",
    "        \n",
    "        # Expand the current root node\n",
    "        curr_state = game.curr_state\n",
    "        value = mcts.expand_root(root, list(range(config['action_space_size'])),\n",
    "                            network, curr_state)\n",
    "        mcts.backpropagate([root], value, config['discount'], min_max_stats)\n",
    "        mcts.add_exploration_noise(config, root)\n",
    "\n",
    "        # Run MCTS\n",
    "        mcts.run_mcts(config, root, network, min_max_stats)\n",
    "\n",
    "        # Select an action to take\n",
    "        action = mcts.select_action(config, root)\n",
    "\n",
    "        # Take an action and store tree search statistics\n",
    "        game.take_action(action, env)\n",
    "        game.store_search_statistics(root)\n",
    "    print(f'Total reward for a train game: {sum(game.reward_history)}')\n",
    "    return game\n",
    "\n",
    "\n",
    "def test(config, network, env, test_log):\n",
    "    \"\"\"\n",
    "    Test performance using trained networks\n",
    "    \"\"\"\n",
    "    mcts = MCTS(config)\n",
    "    returns = 0\n",
    "    for _ in range(config['episodes_per_test']):\n",
    "\n",
    "        start_state, _ = env.reset()\n",
    "        game = Game(config['action_space_size'], config['discount'], start_state)\n",
    "        while not game.done and len(game.action_history) < config['max_moves']:\n",
    "            min_max_stats = MinMaxStats(config['min_value'], config['max_value'])\n",
    "            curr_state = game.curr_state\n",
    "            root = Node(0)\n",
    "            value = mcts.expand_root(root, list(range(config['action_space_size'])),\n",
    "                                network, curr_state)\n",
    "            # don't run mcts.add_exploration_noise for test\n",
    "            mcts.backpropagate([root], value, config['discount'], min_max_stats)\n",
    "            mcts.run_mcts(config, root, network, min_max_stats)\n",
    "            action = mcts.select_action(config, root, test=True) # argmax action selection\n",
    "            game.take_action(action, env)\n",
    "        total_reward = sum(game.reward_history)\n",
    "        print(f'Total reward for a test game: {total_reward}')\n",
    "        test_log.add_reward(total_reward)\n",
    "        returns += total_reward\n",
    "    return returns / config['episodes_per_test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Muzero config setting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we define various hyperparameters. Parameter values are not optimized for this environment so it would be possible to obtain a better performance by adjusting these values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "          # Simulation and environment Config\n",
    "          'action_space_size': 2, # number of action\n",
    "          'state_shape': 4,\n",
    "          'games_per_epoch': 20,\n",
    "          'num_epochs': 25,\n",
    "          'train_per_epoch': 30,\n",
    "          'episodes_per_test': 10,\n",
    "          'cartpole_stop_reward': 200,\n",
    "\n",
    "          'visit_softmax_temperature_fn': 1,\n",
    "          'max_moves': 200,\n",
    "          'num_simulations': 50,\n",
    "          'discount': 0.997,\n",
    "          'min_value': 0,\n",
    "          'max_value': 200,\n",
    "\n",
    "          # Root prior exploration noise.\n",
    "          'root_dirichlet_alpha': 0.1,\n",
    "          'root_exploration_fraction': 0.25,\n",
    "\n",
    "          # UCB parameters\n",
    "          'pb_c_base': 19652,\n",
    "          'pb_c_init': 1.25,\n",
    "\n",
    "          # Model fitting config\n",
    "          'embedding_size': 4,\n",
    "          'hidden_neurons': 48,\n",
    "          'buffer_size': 200,\n",
    "          'batch_size': 512,\n",
    "          'num_unroll_steps': 5,\n",
    "          'td_steps': 10,\n",
    "          'lr_init': 0.01,\n",
    "          }\n",
    "\n",
    "SEED = 0\n",
    "def set_seeds(seed=SEED):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "value_support_size = math.ceil(math.sqrt(config['max_value'])) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Muzero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are ready to run MuZero! Below code train MuZero with Cartpole environment. We can see that each loss gets smaller and the reward gets larger and reaches the max reward (200) as the training progresses. This suggests that MuZero learns the environment dynamics as well as the optimal policy to behave well in the environment!\n",
    "\n",
    "By the way, the policy loss is relatively larger than other losses because there could be multiple optimal actions with the Cartpole environment at each state, and thus it is difficult to determine an optimal action to minimize the loss. Another thing to note is that the total reward earned during training tends to be lower than the total reward during test because action selection during training involves random noise to encourage exploration while no random noise is added in action selection during test phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seeds for reproducibility\n",
    "set_seeds()\n",
    "\n",
    "# Create networks\n",
    "rep_net = RepresentationNetwork(input_size=config['state_shape'], hidden_neurons=config['hidden_neurons'], embedding_size=config['embedding_size']) # representation function\n",
    "val_net = ValueNetwork(input_size=config['state_shape'], hidden_neurons=config['hidden_neurons'], value_support_size=value_support_size) # prediction function\n",
    "pol_net = PolicyNetwork(input_size=config['state_shape'], hidden_neurons=config['hidden_neurons'], action_size=config['action_space_size']) # prediction function\n",
    "dyn_net = DynamicNetwork(input_size=config['state_shape']+config['action_space_size'], hidden_neurons=config['hidden_neurons'], embedding_size=config['embedding_size']) # dynamics function\n",
    "rew_net = RewardNetwork(input_size=config['state_shape']+config['action_space_size'], hidden_neurons=config['hidden_neurons']) # from dynamics function\n",
    "network = Networks(rep_net, val_net, pol_net, dyn_net, rew_net, max_value=config['max_value'])\n",
    "\n",
    "# Create environment\n",
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "# Create buffer to store games\n",
    "replay_buffer = ReplayBuffer(config)\n",
    "self_play(env, config, replay_buffer, network)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this series of notebooks, we learned the MuZero which is a model-based reinforcement learning algorithm. With the Cartpole gym environment, we confirmed that MuZero can learn good policy by learning the environment dynamics from scratch.\n",
    "While the above implementation works okay, the MuZero paper suggested a few more tricks to improve the performance such as Reanalyze. If you are interested in those techniques, I would encourage you to check their paper. Reanalyze is in Appendix H."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
