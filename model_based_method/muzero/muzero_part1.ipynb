{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MuZero: Model-based RL (Part 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In previous notebooks, I introduced various Reinforcement Learning (RL) methods such as [Q-learning](https://medium.com/@ym1942/find-an-optimal-policy-with-finite-markov-decision-process-part3-td-learning-557245c9735c), [Deep Q-learning](https://medium.com/@ym1942/deep-q-learning-dqn-tutorial-with-cartpole-v0-5505dbd2409e), and [Actor-Critic](https://medium.com/@ym1942/policy-gradient-methods-from-reinforce-to-actor-critic-d56ff0f0af0a). These methods typically assume that the dynamics of the environment are known. In other words, we know how the environment changes when taking a certain action, the rewards obtained in each state, and when the episode terminates.\n",
    "\n",
    "These RL methods, which do not require learning the environment's dynamics, are called model-free RL. They are model-free because the agent does not internally have or learn a model of the environment; the only goal is to learn the optimal policy through strategic interaction with the environment. This approach works when the environment's dynamics are defined and known. Board games such as Chess are examples of this, where we know exactly how the board changes with each move and the conditions for winning or losing.\n",
    "\n",
    "However, in real-life scenarios, we often do not know the environment's dynamics and must learn them through interaction. For example, a walking robot must figure out how the environment changes by moving parts of its body in certain ways. This kind of knowledge about the environment is called a \"dynamics model\" in RL. Methods that learn a world \"dynamics model\" through interaction with the environment are called model-based reinforcement learning.\n",
    "\n",
    "In this series of notebooks, we will learn one of such methods called Muzero developed by Google Deep Mind in 2020. Before Muzero, model-based algorithms did not enjoy success over model-free methods because:\n",
    "- Dynamics models are difficult to learn.\n",
    "- Errors in the dynamics model propagate over steps.\n",
    "    - If a dynamics model makes a large prediction error, there will be a significant gap between the actual and predicted agent states, complicating policy learning.\n",
    "- Environments with high-dimensional input spaces (e.g., images in Atari games) are difficult to learn.\n",
    "    - Even small pixel changes can create different environment states, requiring the agent to learn numerous patterns.\n",
    "\n",
    "MuZero is the first algorithm to solve these problems. It learns the dynamics model through interaction and uses it to learn the optimal action to take (i.e., policy).\n",
    "\n",
    "Here is our roadmap to learn MuZero:\n",
    "- Notebook 1: Self-play to collect training data (this notebook)\n",
    "- Notebook 2: Network weight update\n",
    "- Notebook 3: Integrate two components\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Muzero overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before diving into the main topic of this notebook (self-play to collect training data), let's briefly discuss the overview of MuZero to get the big picture.\n",
    "\n",
    "As mentioned earlier, MuZero learns the dynamics model of the environment to learn the optimal policy. Specifically, MuZero utilizes three deep learning models:\n",
    "- Representation function\n",
    "- Dynamics function\n",
    "- Prediction function\n",
    "\n",
    "We will soon talk about what each model does but I would like to note one important thing - **a key difference between MuZero and traditional model-based methods is the use of latent states**. MuZero uses latent states derived from the actual states to efficiently learn the dynamics and optimal policy.\n",
    "\n",
    "The representation function learns the mapping between the actual environment state and the latent state. The goal of this model is to extract a latent state that is helpful to accurately predict policies, values, and rewards. In other words, this function extracts key features of the environment relevant to policies, values, and rewards. Consider an Atari game environment where the input is a grid of pixels. In each frame, many pixel values change. However, changes in background pixels are less important than changes in pixels representing the main target or player. Traditional methods, which use the full input space to learn dynamics, must account for these irrelevant changes. In other words, the model has to learn the dynamics of the entire high-dimensional pixel images. With MuZero, the representation function extracts key features of the environment state, focusing on learning the dynamics related to them, making the learning process more efficient. The representation function receives the raw current state as input and returns the corresponding latent state.\n",
    "\n",
    "The dynamics function learns how the environment changes using the latent state. It receives the current latent state and action, returning the predicted next latent state and the predicted immediate reward at the next latent state. Again, the key difference with MuZero is the use of latent state encoding useful features to predict policies, values, and rewards. The predicted next latent state from the dynamics function does not have any semantic meaning other than containing useful information for prediction.\n",
    "\n",
    "The prediction function predicts the policy and value at a given latent state. It takes the latent state as input and outputs the predicted policy and value. After completing the model learning process, the predicted policy from this function is used when acting in the environment.\n",
    "\n",
    "In summary, what these functions do is that:\n",
    "1. The representation function extracts the latent state of the current state, including all key information about the environment to predict the value, reward, and policy.\n",
    "2. Given the current latent state, the dynamics function is used to run a mental simulation to predict what will happen in the future if the agent takes a certain action from the current state.\n",
    "3. The prediction function predicts the values and actions to take (policy) at each latent state while running the mental simulation with the dynamics function. The value and policy guide the mental simulation with the dynamics function.\n",
    "\n",
    "Remember that when the environment dynamics are known, an agent can use it to run the mental simulation to see what will happen if it takes a certain action. These three models are used to run a mental simulation even when the environment dynamics are unknown. To fit these three models, we need to collect sample experiences. How should we do this? In this notebook, we focus on how to collect the training data to fit these three models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte Carlo Tree Search to collect training samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MuZero uses Monte Carlo Tree Search (MCTS) to collect training samples through self-play. MCTS consists of four steps:\n",
    "1. Selection: Traverse the current tree from the root node using the Upper Confidence Bound (UCB).\n",
    "2. Expansion: Add a child node to the leaf node which is optimally reached through the selection process.\n",
    "3. Simulation: Perform a random simulation from the expanded child node to a terminating state.\n",
    "4. Backpropagation: Update the value of each ancestor node using the expected value.\n",
    "\n",
    "For a more detailed introduction to MCTS, check [this page](https://www.geeksforgeeks.org/ml-monte-carlo-tree-search-mcts/).\n",
    "\n",
    "MuZero modifies MCTS to work with latent states. Another difference is that instead of running a random simulation from the leaf node, MuZero uses bootstrapping, which employs the expected value at a node as the value estimate, similar to [TD-learning](https://medium.com/@ym1942/find-an-optimal-policy-with-finite-markov-decision-process-part3-td-learning-557245c9735c).\n",
    "\n",
    "Let's see the implementation of this process step by step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from IPython import display\n",
    "\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare gym environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this series of notebooks, we utilize the CartPole-v0 environment in Gymnasium.\n",
    "\n",
    "[Learn more about the CartPole-v0 environment](https://gymnasium.farama.org/environments/classic_control/cart_pole/)\n",
    "\n",
    "The goal of this environment is to balance a pole by applying forces in the left and right directions on the cart. It has a discrete action space:\n",
    "- 0: Push cart to the left\n",
    "- 1: Push cart to the right\n",
    "\n",
    "Upon taking an action, either left or right, an agent observes a 4-dimensional state consisting of:\n",
    "- Cart Position\n",
    "- Cart Velocity\n",
    "- Pole Angle\n",
    "- Pole Angular Velocity\n",
    "\n",
    "A reward of +1 is granted to the agent at each step while the pole is kept upright. The maximum reward an agent can earn in a single episode is 200.\n",
    "\n",
    "The episode ends under the following conditions:\n",
    "- Termination: Pole Angle is greater than ±12°\n",
    "- Termination: Cart Position is greater than ±2.4 (center of the cart reaches the edge of the display)\n",
    "- Truncation: Episode length exceeds 200 steps\n",
    "\n",
    "In the code below, I provide an example of the agent randomly exploring this environment over 20 time steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create CartPole environment\n",
    "env = gym.make('CartPole-v0', render_mode='rgb_array')\n",
    "state, _ = env.reset()\n",
    "\n",
    "# Run the environment for 20 steps\n",
    "for i in range(20):\n",
    "    # Display the current state of the environment\n",
    "    plt.imshow(env.render())\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "    \n",
    "    # Choose a random action from the action space\n",
    "    action = env.action_space.sample()\n",
    "    \n",
    "    # Take the chosen action and observe the next state, reward, and termination status\n",
    "    state, reward, terminated, truncated, info = env.step(action)\n",
    "    \n",
    "    # If the episode is terminated or truncated, reset the environment\n",
    "    if terminated or truncated:\n",
    "        state, info = env.reset()\n",
    "\n",
    "# Close the environment after exploration\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code overview for the training sample collection process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code shows the overview of the sample collection process by playing one game. MuZero iteratively runs the below code multiple times to collect experience over many games. In this notebook, we will learn 6 components of this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "          # Simulation and environment Config\n",
    "          'action_space_size': 2, # number of potential action\n",
    "          'max_moves': 200, # max number of action in one game\n",
    "          'num_simulations': 50, # number of MCTS simulation to perform at each root node\n",
    "          'discount': 0.997, # value discount rate used with bootstrapping\n",
    "          'min_value': 0,\n",
    "          'max_value': 200,\n",
    "\n",
    "          # Parameters to balance exploitation and exploration during MCTS\n",
    "          'root_dirichlet_alpha': 0.1, # \n",
    "          'root_exploration_fraction': 0.25,\n",
    "\n",
    "          # Upper Confidence Bound (UCB) hyperparameters (used in the MuZero paper)\n",
    "          'pb_c_base': 19652,\n",
    "          'pb_c_init': 1.25,\n",
    "}\n",
    "\n",
    "def play_game(network, env):\n",
    "    \"\"\"\n",
    "    Plays one game\n",
    "    network: network models used in MCTS unrolling\n",
    "    env: gym environment\n",
    "    \"\"\"\n",
    "    # Initialize environment\n",
    "    start_state, _ = env.reset()\n",
    "\n",
    "    # (1) Create Game Object to store game play log\n",
    "    game = Game(config['action_space_size'], config['discount'], start_state)\n",
    "    \n",
    "    # Play a game using MCTS until game will be done or max_moves will be reached\n",
    "    while not game.done and len(game.action_history) < config['max_moves']:\n",
    "        # (2) Create Node Object to store information about the root node\n",
    "        root = Node(0)\n",
    "    \n",
    "        # (3) Create MinMaxStats Object to normalize values\n",
    "        min_max_stats = MinMaxStats(min_value, max_value)\n",
    "        \n",
    "        # (4) Expand the current root node\n",
    "        curr_state = game.curr_state\n",
    "        expand_root(root, list(range(config['action_space_size'])),\n",
    "                    network, curr_state)\n",
    "        \n",
    "        # (5) Run MCTS\n",
    "        run_mcts(config, root, network, min_max_stats)\n",
    "        \n",
    "        # (6) Select an action to take\n",
    "        action = select_action(config, root, network)        \n",
    "\n",
    "        # Take an action and store tree search statistics\n",
    "        game.take_action(action, env)\n",
    "        game.store_search_statistics(root)\n",
    "    return game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) Create a Game Object to store game play log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to create a Game class object to store logs of self-play. The data observed during self-play such as normalized visit count (i.e. visit probability) and expected value of the root node will be stored and used to train deep learning networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Game:\n",
    "    \"\"\"\n",
    "    A single episode of interaction with the environment.\n",
    "    \"\"\"\n",
    "    def __init__(self, action_space_size, discount, curr_state):\n",
    "\n",
    "        self.action_space_size = action_space_size\n",
    "        self.curr_state = curr_state\n",
    "        self.done = False\n",
    "        self.discount = discount\n",
    "        self.priorities = None\n",
    "\n",
    "        self.state_history = [self.curr_state]\n",
    "        self.action_history = []\n",
    "        self.reward_history = []\n",
    "\n",
    "        self.root_values = []\n",
    "        self.child_visits = []\n",
    "\n",
    "    def take_action(self, action, env):\n",
    "        \"\"\"\n",
    "        Take an action and store the action, reward, and new state\n",
    "        \"\"\"\n",
    "        observation, reward, terminated, truncated, _ = env.step(action)\n",
    "        self.curr_state = observation\n",
    "        self.action_history.append(action)\n",
    "        self.reward_history.append(reward)\n",
    "        self.done = terminated | truncated\n",
    "        if not self.done:\n",
    "            self.state_history.append(self.curr_state)\n",
    "\n",
    "    def store_search_statistics(self, root):\n",
    "        \"\"\"\n",
    "        Stores the search statistics for the root node\n",
    "        \"\"\"\n",
    "        # Stores the normalized root node child visits (i.e. policy target)\n",
    "        sum_visits = sum(child.visit_count for child in root.children.values())\n",
    "        self.child_visits.append(np.array([\n",
    "            root.children[a].visit_count\n",
    "            / sum_visits if a in root.children else 0\n",
    "            for a in range(self.action_space_size)\n",
    "        ]))\n",
    "        \n",
    "        # Stores the root node value, computed from the MCTS (i.e. vlaue target)\n",
    "        self.root_values.append(root.value())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Create a Node Object to store information about the root node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define a Node class to store information about each node during MCTS. This class object contains key attributes to represent each node, such as the total visit count, total value, and the hidden representation of the node. Below is the implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node(object):\n",
    "    \n",
    "    def __init__(self, prior):\n",
    "        \"\"\"\n",
    "        Node in MCTS\n",
    "        prior: The prior policy on the node, computed from policy network\n",
    "        \"\"\"\n",
    "        self.visit_count = 0\n",
    "        self.prior = prior\n",
    "        self.value_sum = 0\n",
    "        self.children = {}\n",
    "        self.hidden_representation = None\n",
    "        self.reward = 0\n",
    "        self.expanded = False\n",
    "\n",
    "    def value(self):\n",
    "        \"\"\"\n",
    "        Compute expected value of a node\n",
    "        \"\"\"\n",
    "        if self.visit_count == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return self.value_sum / self.visit_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) Create a MinMaxStats Object to normalize values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also create a MinMaxStats class object to normalize the observed value. This class stores the max and min values of the environment to transform the observed value into a range between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinMaxStats(object):\n",
    "    \"\"\"\n",
    "    Store the min-max values of the environment to normalize the values\n",
    "    Max value will be 1 and min value will be 0\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, minimum, maximum):\n",
    "        self.maximum = maximum\n",
    "        self.minimum = minimum\n",
    "\n",
    "    def update(self, value: float):\n",
    "        self.maximum = max(self.maximum, value)\n",
    "        self.minimum = min(self.minimum, value)\n",
    "\n",
    "    def normalize(self, value: float) -> float:\n",
    "        if self.maximum > self.minimum:\n",
    "            # We normalize only when we have set the maximum and minimum values.\n",
    "            return (value - self.minimum) / (self.maximum - self.minimum)\n",
    "        return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4) Expand the current root node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start running MCTS, we first expand the root node. expand_root function runs initial_inference function, which uses representation and prediction functions to get the latent representation, predicted policy, and predicted value of the current root node. We will learn the detailed implementation of initial_inference function in the next notebook. \n",
    "\n",
    "The obtained results are stored in Node object representing the current root node. The obtained policy is used as the probability to choose each child node. MuZero uses a Dirichlet random variable to add some randomness to the prior probability of choosing each child. This randomness helps to explore different children during MCTC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_root(node, actions, network, current_state):\n",
    "    \"\"\"\n",
    "    Expand the root node given the current state\n",
    "    \"\"\"\n",
    "    # obtain the latent state of the root node by using representation and prediction functions\n",
    "    observation = torch.tensor(current_state)\n",
    "    transformed_value, reward, policy_logits, hidden_representation = network.initial_inference(observation)\n",
    "    node.hidden_representation = hidden_representation\n",
    "    node.reward = reward\n",
    "    node.visit_count = 1\n",
    "    node.value_sum = transformed_value\n",
    "\n",
    "    # extract softmax policy and set node.policy\n",
    "    softmax_policy = torch.nn.functional.softmax(torch.squeeze(policy_logits))\n",
    "    node.policy = softmax_policy\n",
    "\n",
    "    # instantiate node's children with prior values, obtained from the predicted policy\n",
    "    for action, prob in zip(actions, softmax_policy):\n",
    "        child = Node(prob)\n",
    "        node.children[action] = child\n",
    "    \n",
    "    # add exploration noise\n",
    "    actions = list(node.children.keys())\n",
    "    noise = np.random.dirichlet([config['root_dirichlet_alpha']]*len(actions))\n",
    "    frac = config['root_exploration_fraction']\n",
    "    for a, n in zip(actions, noise):\n",
    "        node.children[a].prior = node.children[a].prior * (1-frac) + n*frac\n",
    "\n",
    "    # set node as expanded\n",
    "    node.expanded = True\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5) Run MCTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run_mcts function is the main function to run MCTS. It mainly runs three steps:\n",
    "1) Starting from the root node, expand the node based on Upper Confidence Bound score until reaching a node, which has not expanded yet (called a leaf node)\n",
    "2) Expand the leaf node by using networks \n",
    "3) Perform backpropagation to update search statistics of each node upto the root node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_mcts(config, root, network, min_max_stats):\n",
    "    \"\"\"\n",
    "    Run the main loop of MCTS for config['num_simulations'] simulations\n",
    "\n",
    "    root: the root node\n",
    "    network: the network\n",
    "    min_max_stats: the min max stats object\n",
    "    \"\"\"\n",
    "    for i in range(config['num_simulations']):\n",
    "        history = []\n",
    "        node = root\n",
    "        search_path = [node]\n",
    "\n",
    "        # 1. expand node until reaching the leaf node\n",
    "        while node.expanded:\n",
    "            action, node = select_child(config, node, min_max_stats)\n",
    "            history.append(action)\n",
    "            search_path.append(node)\n",
    "        parent = search_path[-2]\n",
    "        action = history[-1]\n",
    "        \n",
    "        # 2. expand the leaf node\n",
    "        value = expand_node(node, list(\n",
    "            range(config['action_space_size'])), network, parent.hidden_representation, action)\n",
    "        \n",
    "        # 3. perform backpropagation\n",
    "        backpropagate(search_path, value,\n",
    "                    config['discount'], min_max_stats)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (5)-1. Select child"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Until reaching the leaf node, select_child function chooses a child with the maximum Upper Confidence Bounds (UCB) score to expand the current node. For more detail on the UCB score, please refer to [Appendix B of the MuZero paper](https://arxiv.org/pdf/1911.08265#page=14.33). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_child(config, node, min_max_stats):\n",
    "    \"\"\"\n",
    "    Select a child at an already expanded node\n",
    "    Selection is based on the UCB score\n",
    "    \"\"\"\n",
    "    best_action, best_child = None, None\n",
    "    ucb_compare = -np.inf\n",
    "    for action,child in node.children.items():\n",
    "        ucb = ucb_score(config, node, child, min_max_stats)\n",
    "        if ucb > ucb_compare:\n",
    "            ucb_compare = ucb\n",
    "            best_action = action # action, int\n",
    "            best_child = child # node object\n",
    "    return best_action, best_child\n",
    "\n",
    "\n",
    "def ucb_score(config, parent, child, min_max_stats):\n",
    "    \"\"\"\n",
    "    Compute UCB Score of a child given the parent statistics\n",
    "    \"\"\"\n",
    "    pb_c = np.log((parent.visit_count + config['pb_c_base'] + 1)\n",
    "                / config['pb_c_base']) + config['pb_c_init']\n",
    "    pb_c *= np.sqrt(parent.visit_count) / (child.visit_count + 1)\n",
    "\n",
    "    prior_score = pb_c*child.prior.detach().numpy()\n",
    "    if child.visit_count > 0:\n",
    "        value_score = min_max_stats.normalize(\n",
    "            child.reward + config['discount']*child.value())\n",
    "    else:\n",
    "        value_score = 0\n",
    "    return prior_score + value_score\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (5)-2. Expand node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After reaching the leaf node, expand_node function expands the leaf node. It uses recurrent_inference function, which leverages dynamics and prediction functions to obtain the predicted reward, policy, value, and next latent state. The obtained outputs are used to register the information about the leaf node and children nodes. We will learn the detailed implementation of recurrent_inference function in the next notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_node(node, actions, network, parent_state, parent_action):\n",
    "    \"\"\"\n",
    "    Expand a leaf node given the parent state and action\n",
    "    \"\"\"\n",
    "    # run recurrent inference at the leaf node\n",
    "    transformed_value, reward, policy_logits, hidden_representation = network.recurrent_inference(parent_state, parent_action)\n",
    "    node.hidden_representation = hidden_representation\n",
    "    node.reward = reward\n",
    "\n",
    "    # compute softmax policy and store it to node.policy\n",
    "    softmax_policy = torch.nn.functional.softmax(torch.squeeze(policy_logits))\n",
    "    node.policy = softmax_policy\n",
    "\n",
    "    # instantiate node's children with prior values, obtained from the predicted softmax policy\n",
    "    for action, prob in zip(actions,softmax_policy):\n",
    "        child = Node(prob)\n",
    "        node.children[action] = child\n",
    "\n",
    "    # set node as expanded\n",
    "    node.expanded = True\n",
    "    \n",
    "    return transformed_value\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (5)-3. Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After expanding the leaf node, MuZero runs backpropagation. In backward order, it updates the node statistics (total visit count and total value) of each node upto the root node. The discount rate is used to compute the discounted value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backpropagate(path, value, discount, min_max_stats):\n",
    "    \"\"\"\n",
    "    Update a discounted total value and total visit count\n",
    "    \"\"\"\n",
    "    for node in reversed(path):\n",
    "        node.visit_count += 1\n",
    "        node.value_sum += value \n",
    "        min_max_stats.update(node.value())\n",
    "        value = node.reward + discount * value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6) Select and take an action from the root"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After performing multiple self-play simulations from the root node with run_mcts function, MuZero chooses the actual action to take to move to the next state. It chooses the action based on the visit count of each child. During a network training phase, it chooses action stochastically. During a test phase, it chooses action deterministically based on argmax policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(config, node, test=False):\n",
    "    \"\"\"\n",
    "    Select an action to take\n",
    "    train mode: action selection is performed stochastically\n",
    "    test mode: action selection is performed deterministically (argmax)\n",
    "    \"\"\"\n",
    "    visit_counts = [\n",
    "        (child.visit_count, action) for action, child in node.children.items()\n",
    "    ]\n",
    "    if not test:\n",
    "        action = softmax_sample(visit_counts, 1)\n",
    "    else:\n",
    "        action = softmax_sample(visit_counts, 0)\n",
    "    return action\n",
    "\n",
    "\n",
    "def softmax_sample(visit_counts, temperature):\n",
    "    \"\"\"\n",
    "    Sample an action\n",
    "    \"\"\"\n",
    "    counts_arr = np.array([c[0] for c in visit_counts])\n",
    "    if temperature == 0: # argmax\n",
    "        action_idx = np.argmax(counts_arr)\n",
    "    else:\n",
    "        numerator = np.power(counts_arr, 1/temperature)\n",
    "        denominator = np.sum(numerator)\n",
    "        dist = numerator / denominator\n",
    "        action_idx = np.random.choice(np.arange(len(counts_arr)), p=dist)\n",
    "\n",
    "    return action_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we learned how MuZero collects training samples with self-play using Monte Carlo Tree Search. In the next notebook, we will learn the details of deep learning models used in MuZero."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pokemon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
